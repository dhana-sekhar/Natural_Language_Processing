{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_imdb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswit3/Start_Your_NLP_Career/blob/master/training_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8lxS9LDTvVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "78b1e04a-0989-4afd-9f08-910d6c718f5b"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.3.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (41.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.16.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HEjNZz7PYLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "349498a9-618c-4fe2-f2d6-e96187c62cc1"
      },
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.datasets import imdb\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input,Embedding,Dense,Flatten\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "#from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkSd6moNPnmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(vocab_size,max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    # save np.load\n",
        "    np_load_old = np.load\n",
        "\n",
        "    # modify the default parameters of np.load\n",
        "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "    (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words = vocab_size,index_from = INDEX_FROM)\n",
        "\n",
        "    # restore np.load for future normal usage\n",
        "    np.load = np_load_old\n",
        "\n",
        "    print(len(X_train), len(X_test), len(y_train), len(y_test), \"#####################################\")\n",
        "\n",
        "    return X_train,X_test,y_train,y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pukDaxHTPnrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_for_word_vectors_imdb(X_train):\n",
        "    \"\"\"\n",
        "        Prepares the input\n",
        "\n",
        "        Args:\n",
        "            X_train = tokenized train data\n",
        "\n",
        "        Returns:\n",
        "            sentences = {list} sentences containing words as tokens\n",
        "            word_index = {dict} word and its indexes in whole of imdb corpus\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "    word_to_index = imdb.get_word_index()\n",
        "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] =1\n",
        "    word_to_index[\"<UNK>\"]=2\n",
        "\n",
        "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(X_train)):\n",
        "        temp = [index_to_word[ids] for ids in X_train[i]]\n",
        "        sentences.append(temp)\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    word_indexes = tokenizer.word_index\n",
        "    \"\"\"\n",
        "\n",
        "    #print(sentences[:10],word_to_index,\"sentences[:10],word_to_index[:10]*********************************************\")\n",
        "    return sentences,word_to_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilklcXU3Pnzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def building_word_vector_model(option,sentences,embed_dim,workers,window,y_train):\n",
        "    \"\"\"\n",
        "        Builds the word vector\n",
        "\n",
        "        Args:\n",
        "            type = {bool} 0 for Word2vec. 1 for gensim Fastext. 2 for Fasttext 2018.\n",
        "            sentences = {list} list of tokenized words\n",
        "            embed_dim = {int} embedding dimension of the word vectors\n",
        "            workers = {int} no. of worker threads to train the model (faster training with multicore machines)\n",
        "            window = {int} max distance between current and predicted word\n",
        "            y_train = y_train\n",
        "\n",
        "        Returns:\n",
        "            model = Word2vec/Gensim fastText/ Fastext_2018 model trained on the training corpus\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    if option == 0:\n",
        "        print(\"Training a word2vec model\")\n",
        "        model = Word2Vec(sentences=sentences, size = embed_dim, window = window) # workers = workers,\n",
        "        print(\"Training complete\")\n",
        "\n",
        "    elif option == 1:\n",
        "        print(\"Training a Gensim FastText model\")\n",
        "        model = FastText(sentences=sentences, size = embed_dim, window = window) # workers = workers, \n",
        "        print(\"Training complete\")\n",
        "\n",
        "    elif option == 2:\n",
        "        print(\"Training a Fasttext model from Facebook Research\")\n",
        "        y_train = [\"__label__positive\" if i==1 else \"__label__negative\" for i in y_train]\n",
        "\n",
        "        with open(\"imdb_train.txt\",\"w\") as text_file:\n",
        "            for i in range(len(sentences)):\n",
        "                print(sentences[i],y_train[i],file = text_file)\n",
        "\n",
        "        model = fasttext.train_unsupervised(\"imdb_train.txt\", model = \"skipgram\", lr=0.05, dim=embed_dim, ws=5, epoch=5) \n",
        "        print(\"Training complete\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCle-bBtPn5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f606d16-7fd3-4e67-b408-d7226a44961c"
      },
      "source": [
        "# specify “option” as  0 – Word2vec, 1 – Gensim FastText, 2- Fasttext\n",
        "\n",
        "option = 1\n",
        "\n",
        "embed_dim = 300\n",
        "split_ratio= 0.33\n",
        "max_len= 200\n",
        "vocab_size= 1000\n",
        "trainable_param= False\n",
        "workers = 3,\n",
        "window = 1\n",
        "\n",
        "\n",
        "x_train,x_test,y_train,y_test = load_data(vocab_size,max_len)\n",
        "sentences,word_ix = prepare_data_for_word_vectors_imdb(x_train)\n",
        "model_wv = building_word_vector_model(option,sentences,embed_dim,workers,window,y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 25000 25000 25000 #####################################\n",
            "Training a Gensim FastText model\n",
            "Training complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0knY1YTUXpFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding_input(X_train,X_test,maxlen):\n",
        "    \"\"\"\n",
        "        Pads the input upto considered max length\n",
        "\n",
        "        Args:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "        Returns:\n",
        "            X_train_pad = padded tokenized train data\n",
        "            X_test_pad = padded tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    print(X_train.shape, X_test.shape, \"before padding\")\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train,maxlen=maxlen,padding=\"post\")\n",
        "\n",
        "    X_test_pad = pad_sequences(X_test,maxlen=maxlen,padding=\"post\")\n",
        "\n",
        "    print(X_train_pad.shape, X_test_pad.shape, \"after padding\")\n",
        "\n",
        "    return X_train_pad,X_test_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrgsBB2vPn_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "095b16f0-93e2-4011-af32-d96e51bfec53"
      },
      "source": [
        "x_train_pad,x_test_pad = padding_input(x_train,x_test,max_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) (25000,) before padding\n",
            "(25000, 200) (25000, 200) after padding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiZ4s8_cY13m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_model(embed_dim,X_train_pad,X_test_pad,y_train,y_test,vocab_size,word_index,w2vmodel,trainable_param):\n",
        "    \"\"\"\n",
        "        Builds the classification model for sentiment analysis\n",
        "\n",
        "        Args:\n",
        "            embded_dim = {int} dimension of the word vectors\n",
        "            X_train_pad = padded tokenized train data\n",
        "            X_test_pad = padded tokenized test data\n",
        "            vocab_size = {int} size of the vocabulary\n",
        "            word_index =  {dict} word and its indexes in whole of imdb corpus\n",
        "            w2vmodel = Word2Vec model\n",
        "            trainable_param = {bool} whether to train the word embeddings in the Embedding layer\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size,embed_dim))\n",
        "    print(embedding_matrix.shape, \"embedding_matrix\")\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_vector = w2vmodel[word]\n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i]=embedding_vector\n",
        "        except:\n",
        "            pass\n",
        "    #print(embedding_vector, word, \"embedding_vector, word\")\n",
        "    print(embedding_matrix.shape ,\"embedding_matrix\")       \n",
        "    embed_layer = Embedding(vocab_size,embed_dim,weights =[embedding_matrix],trainable=trainable_param)\n",
        "\n",
        "    input_seq = Input(shape=(X_train_pad.shape[1],))\n",
        "    embed_seq = embed_layer(input_seq)\n",
        "    x = Dense(256,activation =\"relu\")(embed_seq)\n",
        "    x = Flatten()(x)\n",
        "    preds = Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(input_seq,preds)\n",
        "\n",
        "\n",
        "    model.compile(loss=loss,optimizer=optimizer,metrics= metrics)\n",
        "\n",
        "    model.fit(X_train_pad,y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test_pad,y_test))\n",
        "    predictions = model.predict(X_test_pad)\n",
        "    predictions = [0 if i<0.5 else 1 for i in predictions]\n",
        "    print(\"Accuracy: \",accuracy_score(y_test,predictions))\n",
        "    print(\"Classification Report: \",classification_report(y_test,predictions))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au1UQ5KJY0IJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "2d744be6-1685-47f9-e0a7-198f6baf1d2a"
      },
      "source": [
        "epochs = 1\n",
        "batch_size = 1024\n",
        "loss = \"binary_crossentropy\"\n",
        "optimizer = \"adam\"\n",
        "metrics = [\"accuracy\"]\n",
        "      \n",
        "model = classification_model(embed_dim,x_train_pad,x_test_pad,y_train, y_test,vocab_size,word_ix,model_wv,trainable_param)\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 300) embedding_matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "W0717 14:56:39.917273 140633074726784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1000, 300) embedding_matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0717 14:56:39.969289 140633074726784 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/1\n",
            "25000/25000 [==============================] - 96s 4ms/step - loss: 0.7964 - acc: 0.5249 - val_loss: 0.6798 - val_acc: 0.5708\n",
            "Accuracy:  0.57084\n",
            "Classification Report:                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.84      0.66     12500\n",
            "           1       0.66      0.30      0.41     12500\n",
            "\n",
            "    accuracy                           0.57     25000\n",
            "   macro avg       0.60      0.57      0.54     25000\n",
            "weighted avg       0.60      0.57      0.54     25000\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 200, 300)          300000    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 200, 256)          77056     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 51201     \n",
            "=================================================================\n",
            "Total params: 428,257\n",
            "Trainable params: 128,257\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgbsnlQHZ6Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}