{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmo_pretrained_with_imdb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswit3/Start_Your_NLP_Career/blob/master/elmo_with_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Ao7lfWqY6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af8c4aea-29b6-45c2-d6db-b1a14adb10fb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input,Embedding,Dense,Flatten\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDblyYeZoccV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def data_prep_ELMo(train_x,train_y,test_x,test_y,max_len):\n",
        "\n",
        "    INDEX_FROM = 3\n",
        "    word_to_index = imdb.get_word_index()\n",
        "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] =1\n",
        "    word_to_index[\"<UNK>\"]=2\n",
        "\n",
        "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
        "\n",
        "    sentences=[]\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences=[]\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text , dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text,train_label,test_text,test_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C29Z6kS1oulu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(vocab_size,max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    # save np.load\n",
        "    np_load_old = np.load\n",
        "\n",
        "    # modify the default parameters of np.load\n",
        "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "    (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words = vocab_size,index_from = INDEX_FROM)\n",
        "\n",
        "    # restore np.load for future normal usage\n",
        "    np.load = np_load_old\n",
        "\n",
        "    print(X_train.shape, len(X_test), y_train.shape, len(y_test), \"#####################################\")\n",
        "         \n",
        "    return X_train,X_test,y_train,y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXiWPCd4pqat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_prep_ELMo(train_x,train_y,test_x,test_y,max_len):\n",
        "\n",
        "    INDEX_FROM = 3\n",
        "    word_to_index = imdb.get_word_index()\n",
        "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] =1\n",
        "    word_to_index[\"<UNK>\"]=2\n",
        "\n",
        "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
        "\n",
        "    sentences=[]\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences=[]\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text , dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text,train_label,test_text,test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPMfEWEuogHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "847069dd-929e-4009-ea80-755d8414b8b6"
      },
      "source": [
        "embed_dim = 300\n",
        "split_ratio= 0.33\n",
        "max_len= 200\n",
        "vocab_size= 100\n",
        "trainable_param= False\n",
        "workers = 3,\n",
        "window = 1\n",
        "\n",
        "\n",
        "x_train,x_test,y_train,y_test = load_data(vocab_size,max_len)\n",
        "\n",
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,max_len)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) 25000 (25000,) 25000 #####################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUQ17MhIp6-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"epochs\":1,\n",
        "# \"batch_size\":1024,\n",
        "# \"loss\":\"binary_crossentropy\",\n",
        "# \"optimizer\":\"adam\",\n",
        "# \"metrics\":[\"accuracy\"]\n",
        "      \n",
        "def ELMoEmbedding(x):\n",
        "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True)\n",
        "    print(elmo_model, \"elmo loaded successfully\")\n",
        "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
        "\n",
        "  \n",
        "def Classification_model_with_ELMo(X_train_pad,y_train,X_test_pad,y_test):\n",
        "    input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "    embed_seq = layers.Lambda(ELMoEmbedding, output_shape=(1024,))(input_text)\n",
        "    \n",
        "    print(embed_seq, input_text)\n",
        "    \n",
        "    x = Dense(256,activation =\"relu\")(embed_seq)\n",
        "    preds = Dense(1,activation=\"sigmoid\")(x)\n",
        "    model = Model(input_text,preds)\n",
        "\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
        "\n",
        "    model.fit(X_train_pad[:100],y_train[:100],epochs=1,batch_size=64,validation_data=(X_test_pad[:10],y_test[:10]))\n",
        "\n",
        "    predictions = model.predict(X_test_pad)\n",
        "    predictions = [0 if i<0.5 else 1 for i in predictions]\n",
        "    print(\"Accuracy: \",accuracy_score(y_test,predictions))\n",
        "    print(\"Classification Report: \",classification_report(y_test,predictions))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cRGUhc3sDE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "6f41dfa1-ccdf-4fbf-f43c-2c1ba65a91f0"
      },
      "source": [
        "model = Classification_model_with_ELMo(train_text,train_label,test_text,test_label)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 15:03:50.759687 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 15:03:50.776861 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<tensorflow_hub.module.Module object at 0x7f40222b4748> elmo loaded successfully\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0717 15:03:52.186433 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 15:03:52.217591 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0717 15:03:52.242604 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0717 15:03:52.250121 139914399782784 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"lambda_1/module_apply_default/pooling/truediv:0\", shape=(?, 1024), dtype=float32) Tensor(\"input_1:0\", shape=(?, 1), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0717 15:03:52.434345 139914399782784 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples, validate on 10 samples\n",
            "Epoch 1/1\n",
            "100/100 [==============================] - 164s 2s/step - loss: 0.7625 - acc: 0.5000 - val_loss: 0.7457 - val_acc: 0.4000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}